{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Notebook2: Introduction_to_CUDA_Python_with_Numba.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zFiFlXrTutaz"},"source":["# **Introduction to CUDA Python with Numba**\n","\n","Numba is a compiler for Python array and numerical functions that gives you the power to speed up your applications with high performance functions written directly in Python.\n","\n","Numba translates Python functions to optimized machine code at runtime using the industry-standard LLVM compiler library. Numba-compiled numerical algorithms in Python can approach the speeds of C or FORTRAN.\n","\n","You don't need to replace the Python interpreter, run a separate compilation step, or even have a C/C++ compiler installed. Just apply one of the Numba decorators to your Python function, and Numba does the rest.\n","\n","Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions following the CUDA execution model. Kernels written in Numba appear to have direct access to NumPy arrays. NumPy arrays are transferred between the CPU and the GPU automatically.\n","\n","[Documentation](https://numba.readthedocs.io/en/stable/cuda/index.html)\n","\n","#### **Terminology**\n","* **host**: the CPU\n","* **device**: the GPU\n","* **host memory**: the system main memory\n","* **device memory**: onboard memory on a GPU card\n","* **kernels**: a GPU function launched by the host and executed on the device\n","* **device function**: a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)\n","\n","Most CUDA programming facilities exposed by Numba map directly to the CUDA C language offered by NVidia. Therefore, it is recommended you read the official [CUDA C programming guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MCcozMtIqiMJ"},"source":["# Import libraries"]},{"cell_type":"code","metadata":{"id":"uLBqA0-Atq5_"},"source":["import numpy as np\n","import math\n","import random\n","from decimal import Decimal\n","\n","import numba\n","from numba import jit\n","from numba import vectorize \n","from numba import guvectorize\n","from numba import cuda"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ShxKuP6MuY-S"},"source":["Checking Numba version\n"]},{"cell_type":"code","metadata":{"id":"tdnVos7OuV39"},"source":["print(numba.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wi9Jb2xqzaUn"},"source":["# Compiling Python code with @jit\n","\n","The CUDA JIT is a low-level entry point to the CUDA features in Numba. It translates Python functions into [PTX](http://en.wikipedia.org/wiki/Parallel_Thread_Execution) code which execute on the CUDA hardware. The jit decorator is applied to Python functions written in our [Python dialect for CUDA](https://numba.pydata.org/numba-doc/0.13/CUDAPySpec.html). Numba interacts with the [CUDA Driver API](http://docs.nvidia.com/cuda/cuda-driver-api/index.html) to load the PTX onto the CUDA device and execute."]},{"cell_type":"markdown","metadata":{"id":"EC35OoQjuklK"},"source":["## Compiling functions on the CPU\n","Numba provides several utilities for code generation, but its central feature is the numba.jit() decorator with an explicit signature. Using Numba decorator @jit, which creates a normal function for execution on the CPU."]},{"cell_type":"markdown","metadata":{"id":"pLSwOJCsszt6"},"source":[" Let’s start by peeking at the numba.jit string-doc:"]},{"cell_type":"code","metadata":{"id":"vKvE0BDPswcR"},"source":["print(numba.jit.__doc__)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eNpIFFqMtBi6"},"source":["The recommended way to use the @jit decorator is to let Numba decide when and how to optimize. Let's make a compiled version of a function. In this mode, compilation will be deferred until the first function execution. Numba will infer the argument types at call time, and generate optimized code based on this information."]},{"cell_type":"code","metadata":{"id":"UHWsibwZ0yHV"},"source":["nsamples = 2000\n","\n","# JIT compile a python function conforming to the CUDA-Python specification. To define a CUDA kernel:\n","@jit\n","def monte_carlo_pi(nsamples):\n","    acc = 0\n","    for i in range(nsamples):\n","        x = random.random()\n","        y = random.random()\n","        if (x**2 + y**2) < 1.0:\n","            acc += 1\n","    return 4.0 * acc / nsamples"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iBvSQA_e2aGp"},"source":["Let's compare the speed between the compiled and uncompiled versions. **.py_func** attribute that can be used to access the original uncompiled Python function. "]},{"cell_type":"code","metadata":{"id":"AwAZZKU80y0e"},"source":["%timeit monte_carlo_pi.py_func(nsamples)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HFN3hD4A0zAl"},"source":["%timeit monte_carlo_pi(nsamples)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OYRFGJGh2nDy"},"source":["The Numba-compiled version had a significant speed-up!"]},{"cell_type":"markdown","metadata":{"id":"deJvvQSrbimP"},"source":["## Compilation options\n","\n","A number of keyword-only arguments can be passed to the @jit decorator.\n","\n","Numba has two compilation modes: **nopython mode** and **object mode**.\n","\n"," ```nopython mode``` produces much faster code, but has limitations that can force Numba to fall back to the ```object mode```. To prevent Numba from falling back, and instead raise an error, pass ```nopython=True```."]},{"cell_type":"markdown","metadata":{"id":"pmbgM6h24Hmx"},"source":["Let's see nopython mode. The nopython mode will generate the best performance, but has limitations."]},{"cell_type":"code","metadata":{"id":"Jg7iyMX9JzG3"},"source":["#function without error\n","\n","@jit(\"void(f4[:])\",nopython=True)\n","def squared(a):\n","    squared_val = a*a"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8mEK-im0d4Yj"},"source":["\n","Types that can’t be inferred by the compiler in the nopython mode and it will generate an error."]},{"cell_type":"code","metadata":{"id":"WPqVFwR-c11S"},"source":["#function that contains a variable whose type can’t be inferred by the compiler\n","#nopython mode set to True\n","\n","@jit(\"void(f4[:])\",nopython=True)\n","def squared(a):\n","    squared_val = a*a\n","    val = Decimal(100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5gVxFuSQtj6B"},"source":["If we don’t specify anything like in the function below, where nopython mode is not set to True, the compilation is falling back to object mode and produces a warning but not an error."]},{"cell_type":"code","metadata":{"id":"awW0Ijzqd3gM"},"source":["# function that contains a variable whose type can’t be inferred by the compiler\n","# nopython mode not set to True\n","# compilation is falling back to object mode\n","\n","@jit(\"void(f4[:])\")\n","def squared(a):\n","    squared_val = a*a\n","    val = Decimal(100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vxtpI00D3_XH"},"source":["## Type inference\n","\n","The objective of type inference is assigning a type to every single value in the function. The type of a value can either be:\n","\n","* Implicit, in the case of providing an object that will provide its type. For e.g. in literals.\n","* Explicit, in the case of the programmer explicitly writing the type of a given value. For e.g. when a signature is given to numba.jit. That signature explicitly types the arguments.\n","* Inferred, when the type is deduced from an operation and the types of its operands. For e.g. inferring that the type of a + b, when a and b are of type int is going to be an int\n","Type inference is the process by which all the types that are neither implicit nor explicit are deduced."]},{"cell_type":"code","metadata":{"id":"Ph3zzdBp3r7A"},"source":["monte_carlo_pi.inspect_types()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qrACD0Q9kxIU"},"source":["Numba supports generating NumPy ufuncs and gufuncs. In NumPy there are universal functions(ufuncs) and generalized universal functions (gufuncs). "]},{"cell_type":"markdown","metadata":{"id":"q1LiE4Wozj5z"},"source":["# Creating Numpy universal functions"]},{"cell_type":"markdown","metadata":{"id":"kM95lYiZ0fs7"},"source":["## Universal functions\n","\n","Universal functions(ufuncs) are functions that broadcast an elementwise operation across input arrays of varying numbers of dimensions. Most NumPy functions are ufuncs, and Numba makes it easy to compile custom ufuncs using the @vectorize decorator."]},{"cell_type":"code","metadata":{"id":"wYM7ljjUubTa"},"source":["def cpu_sqrt(x):\n","  return math.sqrt(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"shfhyJwVwv8n"},"source":["@vectorize\n","def cpu_numba_sqrt(x):\n","  return math.sqrt(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LQLzjk2XxS_8"},"source":["@vectorize(['float32(float32)'], target='cuda')\n","def gpu_numba_sqrt(x):\n","    return math.sqrt(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p9v2Oq17xb_B"},"source":["%timeit np.sqrt(25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s7RMJZQmxk7B"},"source":["%timeit cpu_sqrt(25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pr0DbIVJxtWB"},"source":["%timeit cpu_numba_sqrt(25)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dpB23pA_x2N6"},"source":["%timeit gpu_numba_sqrt(25)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wzRChAmjylCU"},"source":["Why did we see an increase in the processing time?\n","\n","An overhead is introduced by Numba to each function call that is larger than the function call overhead of Python itself. \n","\n","Functions that are quick to compute are affected by this.\n","\n","Since the function here is too simple to run on the GPU, we experience longer processing time due to data transfer operations taking place.\n","\n","* input data is transferred to the GPU memory\n","* the square root is calculated on the GPU\n","* the resulting value is sent back to the host system"]},{"cell_type":"markdown","metadata":{"id":"cKr6k1Lrf7GH"},"source":["## Generalized ufuncs on the GPU\n","\n","In generalized ufuncs (gufuncs), the calculation can deal with a sub-array of the input array, and return an array of different dimensions."]},{"cell_type":"code","metadata":{"id":"hVAb9_6e5VTG"},"source":["#calculating the sum of elements in each row of the array\n","\n","@guvectorize(['(float32[:], float32[:])'],\n","             '(n)->()',                \n","             target='cuda')\n","def calc_sum(a, out):\n","    sum = 0\n","    for val in a: \n","        sum += val\n","    out[0] = sum"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8hUPmGrFggZz"},"source":["a = np.arange(50).reshape(10, 5).astype(np.float32)\n","a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V6aGey3jgkWK"},"source":["calc_sum(a)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R7ilWQwUz6Gs"},"source":["# Writing Device Functions"]},{"cell_type":"markdown","metadata":{"id":"oZrZKvSAiSAS"},"source":["## Device functions\n","\n","Uptil now we were compiling all our code in single functions but it’s possible to modularize and write clean code with the help of helper functions for the GPU. These are called **device** functions.\n","\n","`add_values:` device function to sum up the elements of the array using the numba.cuda.jit decorator\n","\n","`calc_sum:` gufunc to make use of the add_values device function\n","\n","`calc_avg:` gufunc to calculate the average"]},{"cell_type":"code","metadata":{"id":"_3FwinUKiY90"},"source":["@cuda.jit(device=True)\n","def add_values(a): \n","  sum = 0\n","  for val in a: \n","    sum += val\n","  return sum\n","\n","@guvectorize(['(float32[:], float32[:])'],\n","             '(n)->()',                \n","             target='cuda')\n","def calc_sum(a, out):\n","    out[0] = add_values(a)\n","\n","@guvectorize(['(float32[:], float32[:])'],\n","             '(n)->()',                \n","             target='cuda')\n","def calc_avg(a, out):\n","    out[0] = add_values(a)/len(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKRmxgs3jgQk"},"source":["calc_sum(a)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rO7lxyh6j19a"},"source":["calc_avg(a)"],"execution_count":null,"outputs":[]}]}